{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94659d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "The first task to work with a gym env is to initialise it using gym.make(name_of_env) and reset it using .reset() function. This resets the env to a starting position, with some noise in state. It returns a tuple of the initial state of environment and a dictionary containing info (Not important for the moment).\n",
    "\n",
    "Just as a environment in RL, you can take action based on current state. This is done using env.step(action), and it returns the following 5 values:\n",
    "\n",
    "1. Next State: The state the environment transitioned to after taking the step.\n",
    "\n",
    "2. Reward: Reward received for taking the action\n",
    "\n",
    "3. Terminated: A boolean which is true if the environment terminated after taking the action. The condition for this termination is provided in the documentation of the environment.\n",
    "\n",
    "4. Truncated: A boolean which is true if the environment was truncated after taking the action, usually because we cannot run a environment for infinite time, and hence every environment has a truncation period. More details in the documentation\n",
    "\n",
    "Note: You must call env.reset() after either termination or truncation.\n",
    "\n",
    "5. Info: A dictionary containing information of env.\n",
    "\n",
    "The observation space of Mountation Car is an array of two variables: Position of car(x - coordinate) and velocity of cart. \n",
    "\n",
    "Three actions are possible, as mentioned in documentation\n",
    "'''\n",
    "class QAgent:\n",
    "    def __init__(self, env: str) -> None:\n",
    "        self.env_name = env\n",
    "        self.env = gym.make(env) \n",
    "        self.state = self.env.reset()[0] #Variable to store current state of the environment\n",
    "        \n",
    "        self.observation_space_size = len(self.state) #2 for Mountain Car\n",
    "        \n",
    "        self.actions = self.env.action_space.n # 3 for Mountain Car, represents total number of possible actions\n",
    "        \n",
    "        self.observation_space_low = self.env.observation_space.low #Returns array of length 2, representing the minimum values of position and velocity respectively. Consult documentation for more info.\n",
    "        \n",
    "        self.observation_space_high = self.env.observation_space.high\n",
    "        \n",
    "        #Hyperparameters. Play around with these values!!\n",
    "        \n",
    "        self.discrete_sizes = [25, 25] # Represents how many parts you want to discretize your observation space in. First element represents parts for position of car, and second for velocity of car.\n",
    "        self.alpha = 0.1 # As defined in update rule\n",
    "        self.gamma = 0.95 # As defined in update rule\n",
    "        \n",
    "        self.num_train_episodes = 25000 #Number of episodes to train the model for\n",
    "        self.epsilon = 1 #Initial value for epsilon-greedy behavior\n",
    "        self.num_episodes_decay = 15000 # Number of episodes to act epsilon-greedily for, after which epsilon becomes 0\n",
    "        self.epsilon_decay = self.epsilon / self.num_episodes_decay # Linear decay of epsilon, that is the amount to be decreased from epsilon after every episode termination\n",
    "        \n",
    "        '''\n",
    "        Q-Table. We have provided one way to initialise it, and tried to keep it general, so you even try a different environment. You are adviced to think of other ways you could have initialised it.\n",
    "        \n",
    "        The dimensions of Q-Table must be parts of state-1 x parts of state-2 x actions (why?). It is initialised with random values here.\n",
    "        \n",
    "        * operator opens the array. So *[1,2] represents 1,2. Hence *self.discrete_sizes, self.actions represents 25, 25, 3 here\n",
    "        '''\n",
    "        self.q_table = np.random.uniform(low=-2, high=0, size=(*self.discrete_sizes, self.actions))\n",
    "        \n",
    "    def get_state_index(self, state):\n",
    "        '''\n",
    "        Define a function which gives the index of the state in the Q_Table. Here a simple example to illustrate this task:\n",
    "        \n",
    "        Suppose low for position is 0, and high 2, and discretised it in 20 parts. Then the sections are [0-0.1], [0.1-0.2]...[1.9-2], and index for say position=0.45 will be 4 (in [0.4-0.5])\n",
    "        \n",
    "        The state here is a array of length self.observation_space_size(2). Other necessary variables are initialised in init method. Try to keep this function general for any environment, but you may hardcode the numbers if you feel the task is difficult to generalise.\n",
    "        \n",
    "        Return a tuple containing the indices along each dimension\n",
    "        '''\n",
    "        Indices = []\n",
    "        low = self.observation_space_low\n",
    "        high = self.observation_space_high\n",
    "        for i in range(len(state)):\n",
    "            r = self.discrete_sizes[i]\n",
    "            r = float(high[i] - low[i])/r\n",
    "            s = state[i]\n",
    "            Indices.append(int((s - low[i])/r))\n",
    "        return tuple(Indices)\n",
    "        pass\n",
    "\n",
    "    def update(self, state, action, reward, next_state, is_terminal):\n",
    "        '''\n",
    "        Update the value of q[state, action] in the q-table based on the update rule. \n",
    "        First discretize both the state and next_state to get indices in q-table.\n",
    "        The boolean is_terminal here represents whether the state action pair resulted in termination (NOT TRUNCATION) of environment. In this case, update the value by considering max_a' q(s', a,) = 0 (consult theory for why) and not based on q-table.\n",
    "        '''\n",
    "        state_indices = self.get_state_index(state)\n",
    "        next_state_indices = self.get_state_index(next_state)\n",
    "        \n",
    "        if is_terminal:\n",
    "            self.q_table[state_indices] = self.q_table[state_indices] + self.alpha*(reward - self.q_table[state_indices])\n",
    "            self.state = self.env.reset()[0]\n",
    "            pass\n",
    "        else:\n",
    "            self.q_table[state_indices] = self.q_table[state_indices] + self.alpha*(reward + self.gamma*(np.argmax(self.q_table[next_state_indices])) - self.q_table[state_indices])\n",
    "            pass\n",
    "    \n",
    "    def get_action(self):    \n",
    "        '''\n",
    "        Get the action either greedily, or randomly based on epsilon (You may use self.env.action_space.sample() to get a random action). Return an int representing action, based on self.state. Remember to discretize self.state first\n",
    "        '''\n",
    "        state_indices = self.get_state_index(self.state)\n",
    "        \n",
    "        if (np.random.rand() < self.epsilon):\n",
    "            action = self.env.action_space.sample()\n",
    "            return action\n",
    "        else:\n",
    "            q_values = self.q_table[state_indices]\n",
    "            action = np.argmax(q_values)\n",
    "            return action\n",
    "        pass\n",
    "    \n",
    "    def env_step(self):\n",
    "        '''\n",
    "        Takes a step in the environment and updated q-table\n",
    "        '''\n",
    "        action = self.get_action()\n",
    "        next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "        \n",
    "        self.update(self.state, action, reward, next_state, terminated and not truncated) # terminated and not truncated is true when the episode got terminated but not truncated.\n",
    "        \n",
    "        self.state = next_state\n",
    "        \n",
    "        return terminated or truncated # Represents whether we need to reset the environment\n",
    "    \n",
    "    def agent_eval(self):\n",
    "        '''Visualise the performance of agent'''\n",
    "        eval_env = gym.make(self.env_name, render_mode = \"human\")\n",
    "        done = False\n",
    "        eval_state = eval_env.reset()[0]\n",
    "        while not done:\n",
    "            action = None # Take action based on greedy strategy now\n",
    "            next_state, reward, terminated, truncated, info = eval_env.step(action)\n",
    "            \n",
    "            eval_env.render() #Renders the environment on a window.\n",
    "            \n",
    "            done = terminated or truncated\n",
    "            eval_state = next_state\n",
    "          \n",
    "    def train(self, eval_intervals):\n",
    "        '''Main function to train the agent'''\n",
    "        for episode in range(1, self.num_train_episodes + 1):\n",
    "            done = False\n",
    "            while not done:\n",
    "                done = self.env_step()\n",
    "            self.state = self.env.reset()[0] # Reset environment after end of episode\n",
    "            \n",
    "            self.epsilon = max(0, self.epsilon - self.epsilon_decay) #Update epsilon after every episode\n",
    "            \n",
    "            if episode % eval_intervals == 0:\n",
    "                #Check performance of agent\n",
    "                self.agent_eval()\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    agent = QAgent(\"MountainCar-v0\")\n",
    "    agent.train(eval_intervals=1000) # Change the number to change frequency of evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1e2672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31837340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
